{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd378092",
   "metadata": {},
   "source": [
    "# CUDA编程模型---错误检测，事件及存储单元"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262a18b",
   "metadata": {},
   "source": [
    "#### 通过之前的学习，我们已经初步掌握了利用GPU加速应用程序的方法。接下来，我们针对更多细节加以训练。本次实验课将会介绍：  \n",
    "1. Cuda编程模型中的错误检测\n",
    "2. Cuda编程模型中的事件\n",
    "3. Cuda编程模型中多种类型的存储单元\n",
    "4. 利用共享存储单元来加速矩阵乘法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdc05b",
   "metadata": {},
   "source": [
    "#### 1.Cuda编程模型中的错误检测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917c351",
   "metadata": {},
   "source": [
    "可以查看Cuda error的四个函数：  \n",
    "```C++\n",
    "__host__​__device__​const char* \tcudaGetErrorName ( cudaError_t error )\n",
    "Returns the string representation of an error code enum name.  \n",
    "\n",
    "__host__​__device__​const char* \tcudaGetErrorString ( cudaError_t error )\n",
    "Returns the description string for an error code.  \n",
    "\n",
    "__host__​__device__​cudaError_t \tcudaGetLastError ( void )\n",
    "Returns the last error from a runtime call.  \n",
    "\n",
    "__host__​__device__​cudaError_t \tcudaPeekAtLastError ( void )\n",
    "Returns the last error from a runtime call.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba62a5",
   "metadata": {},
   "source": [
    "这里我们采用第二个，并将其封装在error.cuh文件中：\n",
    "```C++\n",
    "#pragma once\n",
    "#include <stdio.h>\n",
    "\n",
    "#define CHECK(call)                                   \\\n",
    "do                                                    \\\n",
    "{                                                     \\\n",
    "    const cudaError_t error_code = call;              \\\n",
    "    if (error_code != cudaSuccess)                    \\\n",
    "    {                                                 \\\n",
    "        printf(\"CUDA Error:\\n\");                      \\\n",
    "        printf(\"    File:       %s\\n\", __FILE__);     \\\n",
    "        printf(\"    Line:       %d\\n\", __LINE__);     \\\n",
    "        printf(\"    Error code: %d\\n\", error_code);   \\\n",
    "        printf(\"    Error text: %s\\n\",                \\\n",
    "            cudaGetErrorString(error_code));          \\\n",
    "        exit(1);                                      \\\n",
    "    }                                                 \\\n",
    "} while (0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182a688",
   "metadata": {},
   "source": [
    "那么我们就可以在代码中这么使用：\n",
    "```C++\n",
    "    CHECK(cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b16799",
   "metadata": {},
   "source": [
    "接下来，大家在之前做的[matrix_mul.cu](matrix_mul.cu)文件，添加CHECK()，如果遇到麻烦，请参考[result1.cu](result1.cu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b999da",
   "metadata": {},
   "source": [
    "修改Makefile文件，编译程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01683109",
   "metadata": {},
   "source": [
    "执行查看效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee768d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./matrix_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578366c1",
   "metadata": {},
   "source": [
    "修改\n",
    "```C++\n",
    "CHECK(cudaMemcpy(h_c, d_c, (sizeof(int)*m*k), cudaMemcpyDeviceToHost)); \n",
    "```\n",
    "成\n",
    "```C++\n",
    "CHECK(cudaMemcpy(h_c, d_c, (sizeof(int)*m*k*2), cudaMemcpyDeviceToHost));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af1590",
   "metadata": {},
   "source": [
    "再编译一下，并执行查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd53f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ba935",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./matrix_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo /usr/local/cuda/bin/nvprof ./matrix_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b754ac",
   "metadata": {},
   "source": [
    "这时我们就精准的定位了出现错误的文件，位置，以及错误原因"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9cbcd",
   "metadata": {},
   "source": [
    "#### 2. Cuda编程模型中的事件。事件的本质就是一个标记，它与其所在的流内的特定点相关联。可以使用时间来执行以下两个基本任务：\n",
    "- 同步流执行\n",
    "- 监控设备的进展"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ee175",
   "metadata": {},
   "source": [
    "流中的任意点都可以通过API插入事件以及查询事件完成的函数，只有事件所在流中其之前的操作都完成后才能触发事件完成。默认流中设置事件，那么其前面的所有操作都完成时，事件才出发完成。\n",
    "事件就像一个个路标，其本身不执行什么功能，就像我们最原始测试c语言程序的时候插入的无数多个printf一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85403249",
   "metadata": {},
   "source": [
    "创建和销毁： \n",
    "\n",
    "声明:\n",
    "```C++\n",
    "cudaEvent_t event;\n",
    "```\n",
    "创建：\n",
    "```C++\n",
    "cudaError_t cudaEventCreate(cudaEvent_t* event);\n",
    "```\n",
    "销毁：\n",
    "```C++\n",
    "cudaError_t cudaEventDestroy(cudaEvent_t event);\n",
    "```\n",
    "添加事件到当前执行流：\n",
    "```C++\n",
    "cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0);\n",
    "```\n",
    "等待事件完成，设立flag：\n",
    "```C++\n",
    "cudaError_t cudaEventSynchronize(cudaEvent_t event);//阻塞\n",
    "cudaError_t cudaEventQuery(cudaEvent_t event);//非阻塞\n",
    "```\n",
    "当然，我们也可以用它来记录执行的事件：\n",
    "```C++\n",
    "cudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t stop);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c9e2a",
   "metadata": {},
   "source": [
    "接下来，我们就修改matrix_mul.cu程序，来测试一下核函数执行的时间，如果遇到麻烦，请参考[result2.cu](result2.cu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3c6a8",
   "metadata": {},
   "source": [
    "编译并执行程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a510d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./matrix_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo /usr/local/cuda/bin/nvprof ./matrix_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fec182",
   "metadata": {},
   "source": [
    "#### 3.Cuda编程模型中的多种存储单元"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec4069",
   "metadata": {},
   "source": [
    "![gpu_memory](gpu_memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f1ea5",
   "metadata": {},
   "source": [
    "- 寄存器\n",
    "\n",
    "寄存器是速度最快的存储单元，位于GPU芯片的SM上，用于存储局部变量。每个SM（SMX）上有成千上万的32位寄存器，当kernel函数启动后，这些寄存器被分配给指定的线程来使用。由于不同kernel函数需要的寄存器数量也不相等，所以，也有一个规定一个线程的最大寄存器数量是256个。寄存器的最小单位是register file，所以，在很多图上也会用register file来表示。\n",
    "\n",
    "- Local Memory\n",
    "\n",
    "Local Memory本身在硬件中没有特定的存储单元，而是从Global Memory虚拟出来的地址空间。Local Memory是为寄存器无法满足存储需求的情况而设计的，主要是用于存放单线程的大型数组和变量。Local Memory是线程私有的，线程之间是不可见的。由于GPU硬件单位没有Local Memory的存储单元，所以，针对它的访问是比较慢的。从上面的表格中，也可以看到跟Global Memory的访问速度是接近的。\n",
    "\n",
    "- Shared Memory\n",
    "\n",
    "Shared Memory位于GPU芯片上，访问延迟仅次于寄存器。Shared Memory是可以被一个Block中的所有Thread来进行访问的，可以实现Block内的线程间的低开销通信。在SMX中，L1 Cache跟Shared Memory是共享一个64KB的告诉存储单元的，他们之间的大小划分不同的GPU结构不太一样；\n",
    "\n",
    "- Constant Memory\n",
    "\n",
    "Constant Memory类似于Local Memory，也是没有特定的存储单元的，只是Global Memory的虚拟地址。因为它是只读的，所以简化了缓存管理，硬件无需管理复杂的回写策略。Constant Memory启动的条件是同一个warp所有的线程同时访问同样的常量数据。\n",
    "\n",
    "- Global Memory\n",
    "\n",
    "Global Memory在某种意义上等同于GPU显存，kernel函数通过Global Memory来读写显存。Global Memory是kernel函数输入数据和写入结果的唯一来源。\n",
    "\n",
    "- Texture Memory\n",
    "\n",
    "Texture Memory是GPU的重要特性之一，也是GPU编程优化的关键。Texture Memory实际上也是Global Memory的一部分，但是它有自己专用的只读cache。这个cache在浮点运算很有用，Texture Memory是针对2D空间局部性的优化策略，所以thread要获取2D数据就可以使用texture Memory来达到很高的性能。从读取性能的角度跟Constant Memory类似。\n",
    "\n",
    "- Host Memory\n",
    "\n",
    "主机端存储器主要是内存可以分为两类：可分页内存（Pageable）和页面 （Page-Locked 或 Pinned）内存。\n",
    "\n",
    "可分页内存通过操作系统 API(malloc/free) 分配存储器空间，该内存是可以换页的，即内存页可以被置换到磁盘中。可分页内存是不可用使用DMA（Direct Memory Acess)来进行访问的，普通的C程序使用的内存就是这个内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d76c5",
   "metadata": {},
   "source": [
    "#### 4.利用Shrared Memory来优化矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2bc33",
   "metadata": {},
   "source": [
    "![shared_memory](shared_memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5027b3",
   "metadata": {},
   "source": [
    "当我们在处理矩阵乘法时，假设矩阵M(m,k)\\*N(k,n) = P(m,n)。那么，矩阵M中的一个数值m(x,y),就要被grid中所有满足threadIdx.y+blockIdx.y\\*blockDim.y = y的线程从Global Memory中读一次，一共就是K次。那么，我们看到这么多重复读取，就可以把这个变量放在Shared Memory中，极大地减少每次的读取时间。我们可以按照下面的代码来修改martix_mul的核函数："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f2bfa",
   "metadata": {},
   "source": [
    "```C++   \n",
    "__global__ void gpu_matrix_mult_shared(int *d_a, int *d_b, int *d_result, int m, int n, int k) \n",
    "{\n",
    "    __shared__ int tile_a[BLOCK_SIZE][BLOCK_SIZE];\n",
    "    __shared__ int tile_b[BLOCK_SIZE][BLOCK_SIZE];\n",
    "\n",
    "    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n",
    "    int tmp = 0;\n",
    "    int idx;\n",
    "\n",
    "    for (int sub = 0; sub < gridDim.x; ++sub) \n",
    "    {\n",
    "        idx = row * n + sub * BLOCK_SIZE + threadIdx.x;\n",
    "        tile_a[threadIdx.y][threadIdx.x] = row<n && (sub * BLOCK_SIZE + threadIdx.x)<n? d_a[idx]:0;\n",
    "        idx = (sub * BLOCK_SIZE + threadIdx.y) * n + col;\n",
    "        tile_b[threadIdx.y][threadIdx.x] = col<n && (sub * BLOCK_SIZE + threadIdx.y)<n? d_b[idx]:0;\n",
    "        \n",
    "        __syncthreads();\n",
    "        for (int k = 0; k < BLOCK_SIZE; ++k) \n",
    "        {\n",
    "            tmp += tile_a[threadIdx.y][k] * tile_b[k][threadIdx.x];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    if(row < n && col < n)\n",
    "    {\n",
    "        d_result[row * n + col] = tmp;\n",
    "    }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e70cc3",
   "metadata": {},
   "source": [
    "![array_2d](array_2to1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7ec0d",
   "metadata": {},
   "source": [
    "修改[matrix_mul.cu](matrix_mul.cu)文件，利用Makefile编译，并执行。如果遇到困难，请参考[result4.cu](result4.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f146fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./matrix_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103bb54",
   "metadata": {},
   "source": [
    "利用nvprof来查看性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo /usr/local/cuda/bin/nvprof ./matrix_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456f706",
   "metadata": {},
   "source": [
    "课后作业： \n",
    "\n",
    "1. 修改BLOCK_SIZE查看性能变化\n",
    "2. 修改代码，尝试产生bank_conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4334b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
